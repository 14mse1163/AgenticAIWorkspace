{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf44560b",
   "metadata": {},
   "source": [
    "Agentic RAG : be dynamic in which vector databases to use based on query context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9de018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea36a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8ef5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\",\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a74d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...')]\n"
     ]
    }
   ],
   "source": [
    "doc_list = [doc for sublist in docs for doc in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "docs_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "print(docs_splits[:2])  # Print first 2 splits for inspection\n",
    "\n",
    "\n",
    "vecorstore = FAISS.from_documents(docs_splits, OpenAIEmbeddings())\n",
    "\n",
    "retriever = vecorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5a903b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b2005c2d-4494-41ff-af4d-141f3ad413c3', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='f635532f-9835-4ba5-aa85-7e68a6cc1a08', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='45c22f4c-19ad-464c-ab85-57f05cc6a389', metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is Langgraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894ba815",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrivers tool\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search for information about Langgraph. For any questions about Langgraph, you must use this tool!\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "##another vector db for langchain docs\n",
    "\n",
    "urls=[\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials//qa_chat_history/\",\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75e8e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...')]\n"
     ]
    }
   ],
   "source": [
    "doc_list = [doc for sublist in docs for doc in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "docs_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "print(docs_splits[:2])  # Print first 2 splits for inspection\n",
    "\n",
    "\n",
    "vecorstore_langchain = FAISS.from_documents(docs_splits, OpenAIEmbeddings())\n",
    "\n",
    "retriever_langchain = vecorstore_langchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87f84b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrivers tool\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain = create_retriever_tool(\n",
    "    retriever_langchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search for information about Langchain. For any questions about Langchain, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a12c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b457c",
   "metadata": {},
   "source": [
    "## Langgraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "237b6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85408c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking what LangGraph is. Let me start by recalling what I know about LangGraph. It\\'s a framework related to large language models, right? I remember that LangChain and LangGraph are connected. LangGraph is part of the LangChain ecosystem.\\n\\nSo, the first thing I need to explain is that LangGraph is a framework for building stateful, graph-based workflows using large language models. It\\'s designed to help developers create complex applications by managing the flow of data and state between different components. \\n\\nI should mention that it\\'s built on top of LangChain, which means it integrates with other LangChain components like prompts, models, and memory. The key features here are state management and the ability to create workflows as graphs. State management is important because it allows the application to remember previous interactions and maintain context over time.\\n\\nThe user might not be familiar with the term \"graph-based workflow,\" so I should explain that it\\'s about structuring the application as a series of nodes connected in a graph, where each node represents a step in the process. This allows for more flexible and dynamic workflows compared to linear, step-by-step approaches.\\n\\nI should also highlight the use cases. For example, LangGraph can be used for chatbots that need to handle conversations over multiple messages, or for complex applications that require decision-making based on previous interactions. Maybe mention that it handles concurrency and state preservation, which are important for real-time applications.\\n\\nI need to make sure not to confuse LangGraph with other similar tools. Maybe compare it briefly to other workflow management systems but focus on its integration with large language models. Also, note that it\\'s open-source and developed by the LangChain team, which might be relevant for developers looking to contribute or use it.\\n\\nWait, the user might be a developer looking to build an application with LLMs, so they need to know how LangGraph can help structure their code. Emphasize the ease of building modular components, handling state transitions, and the flexibility of the graph structure. Maybe give an example scenario, like a customer support chatbot that uses multiple steps to resolve an issue, maintaining the conversation history as state.\\n\\nAlso, mention the key components: nodes, edges, state, and the event loop. Nodes are functions or operations, edges define transitions, state is the data context, and the event loop processes the workflow. This structure allows for both linear and branching logic, which is essential for complex workflows.\\n\\nI should check if there\\'s any recent updates to LangGraph. Since the knowledge cutoff is October 2023, I need to ensure I\\'m not citing anything beyond that. But I think the core concepts remain consistent. If there are any new features post-2023, I shouldn\\'t mention them unless specified.\\n\\nIn summary, the answer should cover:\\n- Definition of LangGraph as a framework for stateful, graph-based workflows with LLMs.\\n- Its relation to LangChain.\\n- Key features: state management, modular nodes, flexible workflow design.\\n- Use cases and examples.\\n- How it helps in building complex applications with LLMs.\\n- Maybe a brief mention of its architecture components.\\n\\nAvoid technical jargon where possible, but since the user is asking about LangGraph, they might have a technical background. So it\\'s okay to use terms like nodes, edges, state, etc., as long as I explain them.\\n\\nLet me structure the answer step by step to ensure clarity. Start with the definition, then explain the core concepts, how it\\'s used, and some examples. Make sure to connect it back to why it\\'s useful for developers working with LLMs.\\n</think>\\n\\n**LangGraph** is a framework designed to help developers build **stateful, graph-based workflows** using **Large Language Models (LLMs)**. It is part of the **LangChain** ecosystem, which provides tools for managing prompts, models, memory, and workflows in LLM applications. LangGraph specifically focuses on enabling **complex, dynamic workflows** by modeling application logic as a **graph of interconnected nodes**, where each node represents a specific operation or decision point.\\n\\n---\\n\\n### **Key Concepts**\\n1. **State Management**:\\n   - LangGraph maintains **state** across interactions, allowing workflows to retain context (e.g., conversation history in a chatbot) and make decisions based on prior inputs.\\n   - This is critical for applications requiring multi-step reasoning or persistent user sessions.\\n\\n2. **Graph-Based Workflows**:\\n   - Workflows are structured as **directed graphs**:\\n     - **Nodes**: Represent individual steps (e.g., a function call to an LLM, a database lookup, or conditional logic).\\n     - **Edges**: Define transitions between nodes based on inputs, outputs, or conditions.\\n   - This allows for **branching logic**, **loops**, and **reusability** of components.\\n\\n3. **Integration with LangChain**:\\n   - LangGraph works seamlessly with LangChain’s tools (e.g., prompts, models, memory) to create end-to-end LLM-powered applications.\\n   - It abstracts complexity like concurrency, state persistence, and error handling.\\n\\n4. **Event-Driven Processing**:\\n   - Workflows are processed via an **event loop**, which manages the flow of data between nodes and updates the state dynamically.\\n\\n---\\n\\n### **Use Cases**\\n- **Chatbots/Agents**: Handle multi-turn conversations with memory of past interactions.\\n- **Decision Trees**: Build workflows with conditional branching (e.g., \"If user asks X, follow path A; else, path B\").\\n- **Data Pipelines**: Automate tasks requiring multiple LLM calls (e.g., summarizing, extracting, and transforming data).\\n- **Custom Applications**: Complex systems like customer support bots, code generation tools, or game AI with persistent state.\\n\\n---\\n\\n### **Example Workflow**\\nImagine a customer support chatbot:\\n1. **Node A**: Greeting and initial query.\\n2. **Node B**: Use an LLM to classify the issue (e.g., billing, technical support).\\n3. **Node C**: Route to a relevant expert (e.g., if \"billing,\" call a billing API; else, use a technical troubleshooting script).\\n4. **Node D**: Provide a solution and track conversation history in the state.\\n\\n---\\n\\n### **Why Use LangGraph?**\\n- **Modularity**: Reuse node components across workflows.\\n- **Scalability**: Handle concurrent users and long-running processes.\\n- **Flexibility**: Model workflows as linear, branching, or cyclic graphs.\\n- **Developer-Friendly**: Simplifies state management and integration with LLMs.\\n\\n---\\n\\n### **Getting Started**\\nLangGraph is open-source and developed by the LangChain team. To use it, you can:\\n- Install via Python package: `pip install langgraph`.\\n- Build workflows using Python, defining nodes and edges with LangChain’s tools.\\n- Find examples in the [LangGraph documentation](https://python.langchain.com/) or GitHub repository.\\n\\n---\\n\\nIn summary, **LangGraph** empowers developers to create sophisticated, state-aware LLM applications by abstracting workflow complexity into modular, graph-based structures. It’s particularly useful for applications requiring context-aware, dynamic decision-making.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1473, 'prompt_tokens': 13, 'total_tokens': 1486, 'completion_time': 3.395602726, 'completion_tokens_details': None, 'prompt_time': 0.000572417, 'prompt_tokens_details': None, 'queue_time': 0.282625611, 'total_time': 3.396175143}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_efa9879028', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b706c-55d4-73b0-bea7-b96af4401bf0-0', usage_metadata={'input_tokens': 13, 'output_tokens': 1473, 'total_tokens': 1486})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "llm.invoke(\"What is Langgraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d84b432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"Invokes the agent model to generate a response based on the current state. Given\n",
    "    the response, it will decide to retrive using the retriever tools if needed.\n",
    "\n",
    "    args:\n",
    "        state (AgentState): The current state of the agent, including messages.\n",
    "    \n",
    "    returns:\n",
    "        str: The response generated by the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"CALL AGENT MODEL WITH STATE:\", state)\n",
    "\n",
    "    messages= state[\"messages\"]\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba7094df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchainhub as hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers  import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Sequence, Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98cba541",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8829d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8406112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac2ae783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool,retriever_tool_langchain])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c9586e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL AGENT MODEL WITH STATE: {'messages': [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='e6a3b67e-7e02-47da-808d-8fcbcea27bfa')]}\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "CALL AGENT MODEL WITH STATE: {'messages': [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='e6a3b67e-7e02-47da-808d-8fcbcea27bfa'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Langgraph?\" I need to figure out which tool to use here. Let me check the available functions. There\\'s one called retriever_vector_db_blog for Langgraph and another for Langchain. Since the question is about Langgraph, I should use the retriever_vector_db_blog function. The parameters require a query string. The user\\'s query is straightforward, so I\\'ll use \"What is Langgraph?\" as the query argument. Make sure to format the tool call correctly in JSON inside the XML tags.\\n', 'tool_calls': [{'id': 'a1mstjzkm', 'function': {'arguments': '{\"query\":\"What is Langgraph?\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 144, 'prompt_tokens': 266, 'total_tokens': 410, 'completion_time': 0.24613538, 'completion_tokens_details': {'reasoning_tokens': 112}, 'prompt_time': 0.011614195, 'prompt_tokens_details': None, 'queue_time': 0.057007085, 'total_time': 0.257749575}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-0fea-7d20-848a-3692803b3e98-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'What is Langgraph?'}, 'id': 'a1mstjzkm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 266, 'output_tokens': 144, 'total_tokens': 410, 'output_token_details': {'reasoning': 112}}), ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_db_blog', id='5cecbc39-9f1b-4dd2-a73c-67af4e5ac389', tool_call_id='a1mstjzkm'), AIMessage(content='<think>\\nOkay, the user is asking \"What is Langgraph?\" Let me break this down. They probably want a basic definition, but maybe they need more context. Langgraph isn\\'t a common term, so it could be a specific tool, framework, or maybe even a company. I should consider if there are any recent technologies or projects with that name. Wait, there\\'s a Python library called langgraph that\\'s part of the LangChain ecosystem. It\\'s used for building stateful and parallel agents. So the user might be referring to that. But they might not know that, so I should clarify. The original question is straightforward, but perhaps the user wants more details on its purpose, features, or use cases. An improved question could ask for a detailed explanation, including its use cases and how it fits into the LangChain framework. That way, the answer can cover more aspects without being too brief. Let me check if there are other possibilities for Langgraph. Maybe a graph database or a different project? Not that I can recall. So the improved question should probably ask for a detailed explanation, including its role in LangChain, key features, and applications. That would address the user\\'s potential deeper needs beyond just a definition.\\n</think>\\n\\n**Improved Question:**  \\n\"What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?\"  \\n\\n**Reasoning:**  \\nThe original question seeks a definition of LangGraph, but it lacks specificity about its purpose, context, or use cases. The refined question clarifies:  \\n1. **Scope:** Explicitly ties LangGraph to the LangChain ecosystem (a well-known framework for building AI-driven applications).  \\n2. **Functionality:** Highlights key features like *stateful* and *parallel agent workflows*, which are critical for developers evaluating tools.  \\n3. **Audience:** Targets developers by asking how it enables workflows, addressing practical implementation needs.  \\n\\nThis version ensures the answer addresses technical depth, relevance to AI/ML development, and practical applications, avoiding ambiguity.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 422, 'prompt_tokens': 54, 'total_tokens': 476, 'completion_time': 1.010081524, 'completion_tokens_details': None, 'prompt_time': 0.001998568, 'prompt_tokens_details': None, 'queue_time': 0.056375422, 'total_time': 1.012080092}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-1d23-7e41-9852-7c69092c7124-0', usage_metadata={'input_tokens': 54, 'output_tokens': 422, 'total_tokens': 476})]}\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "CALL AGENT MODEL WITH STATE: {'messages': [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='e6a3b67e-7e02-47da-808d-8fcbcea27bfa'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Langgraph?\" I need to figure out which tool to use here. Let me check the available functions. There\\'s one called retriever_vector_db_blog for Langgraph and another for Langchain. Since the question is about Langgraph, I should use the retriever_vector_db_blog function. The parameters require a query string. The user\\'s query is straightforward, so I\\'ll use \"What is Langgraph?\" as the query argument. Make sure to format the tool call correctly in JSON inside the XML tags.\\n', 'tool_calls': [{'id': 'a1mstjzkm', 'function': {'arguments': '{\"query\":\"What is Langgraph?\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 144, 'prompt_tokens': 266, 'total_tokens': 410, 'completion_time': 0.24613538, 'completion_tokens_details': {'reasoning_tokens': 112}, 'prompt_time': 0.011614195, 'prompt_tokens_details': None, 'queue_time': 0.057007085, 'total_time': 0.257749575}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-0fea-7d20-848a-3692803b3e98-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'What is Langgraph?'}, 'id': 'a1mstjzkm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 266, 'output_tokens': 144, 'total_tokens': 410, 'output_token_details': {'reasoning': 112}}), ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_db_blog', id='5cecbc39-9f1b-4dd2-a73c-67af4e5ac389', tool_call_id='a1mstjzkm'), AIMessage(content='<think>\\nOkay, the user is asking \"What is Langgraph?\" Let me break this down. They probably want a basic definition, but maybe they need more context. Langgraph isn\\'t a common term, so it could be a specific tool, framework, or maybe even a company. I should consider if there are any recent technologies or projects with that name. Wait, there\\'s a Python library called langgraph that\\'s part of the LangChain ecosystem. It\\'s used for building stateful and parallel agents. So the user might be referring to that. But they might not know that, so I should clarify. The original question is straightforward, but perhaps the user wants more details on its purpose, features, or use cases. An improved question could ask for a detailed explanation, including its use cases and how it fits into the LangChain framework. That way, the answer can cover more aspects without being too brief. Let me check if there are other possibilities for Langgraph. Maybe a graph database or a different project? Not that I can recall. So the improved question should probably ask for a detailed explanation, including its role in LangChain, key features, and applications. That would address the user\\'s potential deeper needs beyond just a definition.\\n</think>\\n\\n**Improved Question:**  \\n\"What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?\"  \\n\\n**Reasoning:**  \\nThe original question seeks a definition of LangGraph, but it lacks specificity about its purpose, context, or use cases. The refined question clarifies:  \\n1. **Scope:** Explicitly ties LangGraph to the LangChain ecosystem (a well-known framework for building AI-driven applications).  \\n2. **Functionality:** Highlights key features like *stateful* and *parallel agent workflows*, which are critical for developers evaluating tools.  \\n3. **Audience:** Targets developers by asking how it enables workflows, addressing practical implementation needs.  \\n\\nThis version ensures the answer addresses technical depth, relevance to AI/ML development, and practical applications, avoiding ambiguity.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 422, 'prompt_tokens': 54, 'total_tokens': 476, 'completion_time': 1.010081524, 'completion_tokens_details': None, 'prompt_time': 0.001998568, 'prompt_tokens_details': None, 'queue_time': 0.056375422, 'total_time': 1.012080092}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-1d23-7e41-9852-7c69092c7124-0', usage_metadata={'input_tokens': 54, 'output_tokens': 422, 'total_tokens': 476}), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user asked, \"What is Langgraph?\" and I used the retriever tool to find information. The initial response was just a bunch of redirects, so I tried to improve the question to get a better answer. Now, the user hasn\\'t provided new input, so I need to figure out the next step.\\n\\nFirst, I should check if the improved question I suggested actually gets answered. Since the original tool response wasn\\'t helpful, maybe I need to use the retriever again with the improved question. Wait, the tools available are retriever_vector_db_blog and retriever_vector_langchain_blog. Since LangGraph is part of LangChain, maybe the correct tool is retriever_vector_langchain_blog. Let me try using that with the improved question.\\n\\nSo, the improved question was: \"What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?\" I should call the retriever_vector_langchain_blog with this query to get detailed information. That should provide the necessary details about LangGraph\\'s role in LangChain, its features like stateful and parallel workflows, and how developers can use it. I\\'ll make sure the tool call is correctly formatted and then present the answer once I get the response.\\n', 'tool_calls': [{'id': '47yhm0b2e', 'function': {'arguments': '{\"query\":\"What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 312, 'prompt_tokens': 505, 'total_tokens': 817, 'completion_time': 0.605546189, 'completion_tokens_details': {'reasoning_tokens': 258}, 'prompt_time': 0.022540299, 'prompt_tokens_details': None, 'queue_time': 0.159201361, 'total_time': 0.628086488}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-24d0-71d3-9608-eb0bf2121a62-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?'}, 'id': '47yhm0b2e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 505, 'output_tokens': 312, 'total_tokens': 817, 'output_token_details': {'reasoning': 258}}), ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_langchain_blog', id='73f58201-93bf-4ca5-8a1c-9e9093e33344', tool_call_id='47yhm0b2e'), AIMessage(content='<think>\\nOkay, the user is asking \"What is Langgraph?\" Let me break this down.\\n\\nFirst, I need to understand what Langgraph is. The name suggests it\\'s related to both language and graphs. So maybe it\\'s a tool or framework that uses graph structures for language processing tasks. But I\\'m not entirely sure. The user probably wants a concise definition.\\n\\nThey might be looking for a project or library that helps in NLP tasks using graph-based methods. Maybe it\\'s similar to how some machine learning models use graphs for better representation of language data. Alternatively, it could be a visualization tool for language models. \\n\\nThe user\\'s question is straightforward, but to improve it, I should consider if they want technical details, use cases, or a comparison with other tools. The improved question should ask for a clear definition and maybe the purpose or application areas of Langgraph. That way, the answer can address both what it is and why it\\'s used. \\n\\nI should phrase the improved question to elicit a comprehensive response. Maybe something like, \"What is Langgraph, and what are its main applications in natural language processing?\" That covers the definition and the practical uses, which gives the user a fuller understanding.\\n</think>\\n\\n**Improved Question:**  \\n\"What is Langgraph, and what are its key features or applications in the context of language processing or machine learning?\"  \\n\\n**Reasoning:**  \\nThe original question (\"What is Langgraph?\") is clear but could benefit from additional specificity to elicit a more detailed and actionable response. The improved question explicitly asks for both a definition of Langgraph and its practical relevance (e.g., features, applications, or use cases), which helps clarify its purpose, functionality, and significance in fields like natural language processing (NLP) or machine learning. This encourages a deeper exploration of the concept beyond a surface-level explanation.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 378, 'prompt_tokens': 54, 'total_tokens': 432, 'completion_time': 0.890609206, 'completion_tokens_details': None, 'prompt_time': 0.008440854, 'prompt_tokens_details': None, 'queue_time': 0.058199106, 'total_time': 0.89905006}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-3468-7d52-bcad-5173a9711d75-0', usage_metadata={'input_tokens': 54, 'output_tokens': 378, 'total_tokens': 432})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='e6a3b67e-7e02-47da-808d-8fcbcea27bfa'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Langgraph?\" I need to figure out which tool to use here. Let me check the available functions. There\\'s one called retriever_vector_db_blog for Langgraph and another for Langchain. Since the question is about Langgraph, I should use the retriever_vector_db_blog function. The parameters require a query string. The user\\'s query is straightforward, so I\\'ll use \"What is Langgraph?\" as the query argument. Make sure to format the tool call correctly in JSON inside the XML tags.\\n', 'tool_calls': [{'id': 'a1mstjzkm', 'function': {'arguments': '{\"query\":\"What is Langgraph?\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 144, 'prompt_tokens': 266, 'total_tokens': 410, 'completion_time': 0.24613538, 'completion_tokens_details': {'reasoning_tokens': 112}, 'prompt_time': 0.011614195, 'prompt_tokens_details': None, 'queue_time': 0.057007085, 'total_time': 0.257749575}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-0fea-7d20-848a-3692803b3e98-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'What is Langgraph?'}, 'id': 'a1mstjzkm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 266, 'output_tokens': 144, 'total_tokens': 410, 'output_token_details': {'reasoning': 112}}),\n",
       "  ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_db_blog', id='5cecbc39-9f1b-4dd2-a73c-67af4e5ac389', tool_call_id='a1mstjzkm'),\n",
       "  AIMessage(content='<think>\\nOkay, the user is asking \"What is Langgraph?\" Let me break this down. They probably want a basic definition, but maybe they need more context. Langgraph isn\\'t a common term, so it could be a specific tool, framework, or maybe even a company. I should consider if there are any recent technologies or projects with that name. Wait, there\\'s a Python library called langgraph that\\'s part of the LangChain ecosystem. It\\'s used for building stateful and parallel agents. So the user might be referring to that. But they might not know that, so I should clarify. The original question is straightforward, but perhaps the user wants more details on its purpose, features, or use cases. An improved question could ask for a detailed explanation, including its use cases and how it fits into the LangChain framework. That way, the answer can cover more aspects without being too brief. Let me check if there are other possibilities for Langgraph. Maybe a graph database or a different project? Not that I can recall. So the improved question should probably ask for a detailed explanation, including its role in LangChain, key features, and applications. That would address the user\\'s potential deeper needs beyond just a definition.\\n</think>\\n\\n**Improved Question:**  \\n\"What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?\"  \\n\\n**Reasoning:**  \\nThe original question seeks a definition of LangGraph, but it lacks specificity about its purpose, context, or use cases. The refined question clarifies:  \\n1. **Scope:** Explicitly ties LangGraph to the LangChain ecosystem (a well-known framework for building AI-driven applications).  \\n2. **Functionality:** Highlights key features like *stateful* and *parallel agent workflows*, which are critical for developers evaluating tools.  \\n3. **Audience:** Targets developers by asking how it enables workflows, addressing practical implementation needs.  \\n\\nThis version ensures the answer addresses technical depth, relevance to AI/ML development, and practical applications, avoiding ambiguity.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 422, 'prompt_tokens': 54, 'total_tokens': 476, 'completion_time': 1.010081524, 'completion_tokens_details': None, 'prompt_time': 0.001998568, 'prompt_tokens_details': None, 'queue_time': 0.056375422, 'total_time': 1.012080092}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-1d23-7e41-9852-7c69092c7124-0', usage_metadata={'input_tokens': 54, 'output_tokens': 422, 'total_tokens': 476}),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user asked, \"What is Langgraph?\" and I used the retriever tool to find information. The initial response was just a bunch of redirects, so I tried to improve the question to get a better answer. Now, the user hasn\\'t provided new input, so I need to figure out the next step.\\n\\nFirst, I should check if the improved question I suggested actually gets answered. Since the original tool response wasn\\'t helpful, maybe I need to use the retriever again with the improved question. Wait, the tools available are retriever_vector_db_blog and retriever_vector_langchain_blog. Since LangGraph is part of LangChain, maybe the correct tool is retriever_vector_langchain_blog. Let me try using that with the improved question.\\n\\nSo, the improved question was: \"What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?\" I should call the retriever_vector_langchain_blog with this query to get detailed information. That should provide the necessary details about LangGraph\\'s role in LangChain, its features like stateful and parallel workflows, and how developers can use it. I\\'ll make sure the tool call is correctly formatted and then present the answer once I get the response.\\n', 'tool_calls': [{'id': '47yhm0b2e', 'function': {'arguments': '{\"query\":\"What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 312, 'prompt_tokens': 505, 'total_tokens': 817, 'completion_time': 0.605546189, 'completion_tokens_details': {'reasoning_tokens': 258}, 'prompt_time': 0.022540299, 'prompt_tokens_details': None, 'queue_time': 0.159201361, 'total_time': 0.628086488}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-24d0-71d3-9608-eb0bf2121a62-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is LangGraph, and how does it function within the LangChain ecosystem to enable stateful and parallel agent workflows for developers?'}, 'id': '47yhm0b2e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 505, 'output_tokens': 312, 'total_tokens': 817, 'output_token_details': {'reasoning': 258}}),\n",
       "  ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_langchain_blog', id='73f58201-93bf-4ca5-8a1c-9e9093e33344', tool_call_id='47yhm0b2e'),\n",
       "  AIMessage(content='<think>\\nOkay, the user is asking \"What is Langgraph?\" Let me break this down.\\n\\nFirst, I need to understand what Langgraph is. The name suggests it\\'s related to both language and graphs. So maybe it\\'s a tool or framework that uses graph structures for language processing tasks. But I\\'m not entirely sure. The user probably wants a concise definition.\\n\\nThey might be looking for a project or library that helps in NLP tasks using graph-based methods. Maybe it\\'s similar to how some machine learning models use graphs for better representation of language data. Alternatively, it could be a visualization tool for language models. \\n\\nThe user\\'s question is straightforward, but to improve it, I should consider if they want technical details, use cases, or a comparison with other tools. The improved question should ask for a clear definition and maybe the purpose or application areas of Langgraph. That way, the answer can address both what it is and why it\\'s used. \\n\\nI should phrase the improved question to elicit a comprehensive response. Maybe something like, \"What is Langgraph, and what are its main applications in natural language processing?\" That covers the definition and the practical uses, which gives the user a fuller understanding.\\n</think>\\n\\n**Improved Question:**  \\n\"What is Langgraph, and what are its key features or applications in the context of language processing or machine learning?\"  \\n\\n**Reasoning:**  \\nThe original question (\"What is Langgraph?\") is clear but could benefit from additional specificity to elicit a more detailed and actionable response. The improved question explicitly asks for both a definition of Langgraph and its practical relevance (e.g., features, applications, or use cases), which helps clarify its purpose, functionality, and significance in fields like natural language processing (NLP) or machine learning. This encourages a deeper exploration of the concept beyond a surface-level explanation.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 378, 'prompt_tokens': 54, 'total_tokens': 432, 'completion_time': 0.890609206, 'completion_tokens_details': None, 'prompt_time': 0.008440854, 'prompt_tokens_details': None, 'queue_time': 0.058199106, 'total_time': 0.89905006}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-3468-7d52-bcad-5173a9711d75-0', usage_metadata={'input_tokens': 54, 'output_tokens': 378, 'total_tokens': 432}),\n",
       "  AIMessage(content=\"It seems the tools provided are not successfully retrieving information about LangGraph at the moment. Would you like me to provide a general explanation based on common knowledge, or is there another specific aspect you'd like me to clarify?\", additional_kwargs={'reasoning_content': \"Okay, the user is asking about Langgraph, but previous attempts to get information using the retriever tools just resulted in redirecting messages. The improved questions suggested by the assistant also didn't work. Maybe the issue is that the tools aren't correctly accessing the necessary databases or blogs. Let me check the function names again. The first tool is retriever_vector_db_blog for Langgraph and retriever_vector_langchain_blog for Langchain. Wait, the user is asking about Langgraph, which might be part of the LangChain ecosystem. The second tool is for Langchain, but maybe the correct one is the first one. But in the second attempt, the assistant used the Langchain retriever, which might not be right. The redirecting could be happening because the query isn't matching the available data. Alternatively, maybe there's a misconfiguration in the tools. Since the user is still getting redirects, perhaps the tools don't have the necessary information. The assistant should inform the user that the tools aren't providing results and offer to help with a general explanation instead.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 265, 'prompt_tokens': 726, 'total_tokens': 991, 'completion_time': 0.637685874, 'completion_tokens_details': {'reasoning_tokens': 216}, 'prompt_time': 0.029877841, 'prompt_tokens_details': None, 'queue_time': 0.056135419, 'total_time': 0.667563715}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-3be9-78e1-9c23-78eeec2513ce-0', usage_metadata={'input_tokens': 726, 'output_tokens': 265, 'total_tokens': 991, 'output_token_details': {'reasoning': 216}})]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Langgraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "483cde75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL AGENT MODEL WITH STATE: {'messages': [HumanMessage(content='What is python?', additional_kwargs={}, response_metadata={}, id='c5e208b8-bcfa-47b1-a401-23d0ca27f794')]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is python?', additional_kwargs={}, response_metadata={}, id='c5e208b8-bcfa-47b1-a401-23d0ca27f794'),\n",
       "  AIMessage(content='Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python emphasizes code readability with minimal syntax, making it an excellent choice for beginners and experienced developers alike. \\n\\nKey features include:\\n- **General-purpose**: Used for web development, data analysis, artificial intelligence, scientific computing, automation, and more.\\n- **Cross-platform**: Works on various operating systems (Windows, macOS, Linux).\\n- **Extensive libraries**: A vast ecosystem of libraries and frameworks (e.g., Django, NumPy, Pandas, TensorFlow).\\n- **Dynamic typing**: Variables do not require explicit declaration of data types.\\n- **Community support**: A large and active community contributes to its growth and resources.\\n\\nIt is widely adopted in fields like machine learning, DevOps, and scripting due to its flexibility and ease of use.', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What is Python?\" Let me think about how to approach this.\\n\\nFirst, I need to check if the question is related to Langgraph or Langchain, since those are the tools I have. But \"What is Python?\" is a general question about the programming language itself, not specifically about those tools. \\n\\nThe user probably wants a basic explanation of Python, its uses, maybe some history. Since the tools provided are for retrieving info on Langgraph and Langchain, and the question isn\\'t about those, I shouldn\\'t use the functions. \\n\\nI should answer directly, without calling any functions. Just provide a concise definition and key points about Python. Make sure to mention it\\'s a high-level, interpreted language, used for web dev, data science, automation, etc. Also note its readability and popularity.\\n\\nNo need to use the retriever tools here because the question isn\\'t about Langgraph or Langchain. The tools are for when the user asks about those specific frameworks. So, respond normally.\\n'}, response_metadata={'token_usage': {'completion_tokens': 398, 'prompt_tokens': 265, 'total_tokens': 663, 'completion_time': 1.2253776809999999, 'completion_tokens_details': {'reasoning_tokens': 209}, 'prompt_time': 0.01214382, 'prompt_tokens_details': None, 'queue_time': 0.158869439, 'total_time': 1.237521501}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7097-f19b-74b0-b718-c53089f0959a-0', usage_metadata={'input_tokens': 265, 'output_tokens': 398, 'total_tokens': 663, 'output_token_details': {'reasoning': 209}})]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is python?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3481715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL AGENT MODEL WITH STATE: {'messages': [HumanMessage(content='What is langchain?', additional_kwargs={}, response_metadata={}, id='6959af73-078a-4789-8507-4ce17fa656ef')]}\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "CALL AGENT MODEL WITH STATE: {'messages': [HumanMessage(content='What is langchain?', additional_kwargs={}, response_metadata={}, id='6959af73-078a-4789-8507-4ce17fa656ef'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What is langchain?\" I need to figure out which tool to use here. Looking at the provided functions, there\\'s one for Langgraph and one for Langchain. Since the question is about Langchain, the correct tool would be retriever_vector_langchain_blog. The parameters require a query, so I\\'ll use \"What is langchain?\" as the query argument. I should make sure to format the tool call correctly within the XML tags.\\n', 'tool_calls': [{'id': 'xbyvzy67f', 'function': {'arguments': '{\"query\":\"What is langchain?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 266, 'total_tokens': 396, 'completion_time': 0.202621876, 'completion_tokens_details': {'reasoning_tokens': 97}, 'prompt_time': 0.010891876, 'prompt_tokens_details': None, 'queue_time': 0.051127434, 'total_time': 0.213513752}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7098-348d-7141-a1bd-4ca24d23303a-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is langchain?'}, 'id': 'xbyvzy67f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 266, 'output_tokens': 130, 'total_tokens': 396, 'output_token_details': {'reasoning': 97}}), ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_langchain_blog', id='941409fd-2753-4349-a43f-285fbdb4debe', tool_call_id='xbyvzy67f'), AIMessage(content='<think>\\nOkay, let\\'s see. The user asked \"What is langchain?\" I need to figure out the underlying intent here. Langchain is probably a technology or framework related to programming or data processing. The user might be looking for a definition, use cases, or how to use it.\\n\\nHmm, maybe they\\'re a developer trying to integrate LangChain into their project. They might want to know its main features, what problem it solves, or how it compares to other tools. The original question is pretty broad, so the improved question should be more specific. Instead of asking what it is, asking about its purpose and use cases would give a clearer answer. Also, including whether it\\'s a library or a framework could help narrow down the response. That makes sense. Let me rephrase the question to cover those aspects.\\n</think>\\n\\n**Improved Question:**  \\n\"Can you explain the primary purpose and key features of LangChain, and how it is typically used in applications like natural language processing or machine learning workflows?\"  \\n\\n**Reasoning:**  \\nThe improved question clarifies the intent by focusing on LangChain\\'s functional role (purpose), its distinguishing characteristics (key features), and practical applications (e.g., NLP, ML workflows). This avoids ambiguity and guides the response to address both conceptual understanding and use cases.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 266, 'prompt_tokens': 54, 'total_tokens': 320, 'completion_time': 0.625891814, 'completion_tokens_details': None, 'prompt_time': 0.002054216, 'prompt_tokens_details': None, 'queue_time': 0.158332194, 'total_time': 0.62794603}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7098-43c7-70e0-9c15-f0874d1c726d-0', usage_metadata={'input_tokens': 54, 'output_tokens': 266, 'total_tokens': 320})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is langchain?', additional_kwargs={}, response_metadata={}, id='6959af73-078a-4789-8507-4ce17fa656ef'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What is langchain?\" I need to figure out which tool to use here. Looking at the provided functions, there\\'s one for Langgraph and one for Langchain. Since the question is about Langchain, the correct tool would be retriever_vector_langchain_blog. The parameters require a query, so I\\'ll use \"What is langchain?\" as the query argument. I should make sure to format the tool call correctly within the XML tags.\\n', 'tool_calls': [{'id': 'xbyvzy67f', 'function': {'arguments': '{\"query\":\"What is langchain?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 266, 'total_tokens': 396, 'completion_time': 0.202621876, 'completion_tokens_details': {'reasoning_tokens': 97}, 'prompt_time': 0.010891876, 'prompt_tokens_details': None, 'queue_time': 0.051127434, 'total_time': 0.213513752}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7098-348d-7141-a1bd-4ca24d23303a-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is langchain?'}, 'id': 'xbyvzy67f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 266, 'output_tokens': 130, 'total_tokens': 396, 'output_token_details': {'reasoning': 97}}),\n",
       "  ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_langchain_blog', id='941409fd-2753-4349-a43f-285fbdb4debe', tool_call_id='xbyvzy67f'),\n",
       "  AIMessage(content='<think>\\nOkay, let\\'s see. The user asked \"What is langchain?\" I need to figure out the underlying intent here. Langchain is probably a technology or framework related to programming or data processing. The user might be looking for a definition, use cases, or how to use it.\\n\\nHmm, maybe they\\'re a developer trying to integrate LangChain into their project. They might want to know its main features, what problem it solves, or how it compares to other tools. The original question is pretty broad, so the improved question should be more specific. Instead of asking what it is, asking about its purpose and use cases would give a clearer answer. Also, including whether it\\'s a library or a framework could help narrow down the response. That makes sense. Let me rephrase the question to cover those aspects.\\n</think>\\n\\n**Improved Question:**  \\n\"Can you explain the primary purpose and key features of LangChain, and how it is typically used in applications like natural language processing or machine learning workflows?\"  \\n\\n**Reasoning:**  \\nThe improved question clarifies the intent by focusing on LangChain\\'s functional role (purpose), its distinguishing characteristics (key features), and practical applications (e.g., NLP, ML workflows). This avoids ambiguity and guides the response to address both conceptual understanding and use cases.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 266, 'prompt_tokens': 54, 'total_tokens': 320, 'completion_time': 0.625891814, 'completion_tokens_details': None, 'prompt_time': 0.002054216, 'prompt_tokens_details': None, 'queue_time': 0.158332194, 'total_time': 0.62794603}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7098-43c7-70e0-9c15-f0874d1c726d-0', usage_metadata={'input_tokens': 54, 'output_tokens': 266, 'total_tokens': 320}),\n",
       "  AIMessage(content=\"LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs). It provides tools for tasks like data loading, model integration, memory management, agent workflows, and deployment. Key features include:\\n\\n1. **Modular Components**: Pre-built modules for common tasks (e.g., text summarization, question-answering).\\n2. **Integration**: Supports multiple LLM providers (e.g., OpenAI, Hugging Face) and databases.\\n3. **Agents & Memory**: Enables dynamic decision-making (agents) and persistent memory across interactions.\\n4. **Customization**: Allows users to chain operations, add logic, and build custom workflows.\\n\\nIt is commonly used in NLP applications, ML pipelines, and conversational AI to streamline development and deployment. \\n\\n*(Note: The tool for retrieving this info appears to be redirecting. Let me know if you'd like a deeper explanation of any specific aspect!)*\", additional_kwargs={'reasoning_content': 'Okay, the user initially asked, \"What is langchain?\" and I used the retriever_vector_langchain_blog tool to find information. The response they got was a series of redirecting messages, which didn\\'t provide useful information. Now, the user hasn\\'t provided a new query yet, but I need to figure out how to handle this situation.\\n\\nFirst, I should acknowledge that the previous tool call didn\\'t work as expected. The redirects suggest that the function might not be functioning correctly or the query didn\\'t retrieve the necessary data. Since the user\\'s original question was about LangChain, I need to find another way to answer it without relying on the faulty tool.\\n\\nI remember that the user\\'s question was about LangChain, and the first tool call was supposed to retrieve information from the LangChain blog. The fact that it\\'s redirecting might mean that the database or the function isn\\'t properly connected. In such a case, I should consider using another method to provide an answer. However, since the available tools are limited to the two functions provided, and the user\\'s query is specifically about LangChain, I might have to rely on the other function, but wait, the other function is for Langgraph. The user isn\\'t asking about Langgraph, so that\\'s not helpful.\\n\\nGiven that, maybe the system is expecting me to recognize that the initial tool call failed and to try again with a modified query. Alternatively, perhaps the user needs to clarify their question. However, in the current scenario, the user hasn\\'t provided a new question yet. The assistant\\'s job is to handle the previous interaction where the tool response was unhelpful. Since the user\\'s original question was unanswered, the assistant should now provide a direct answer based on existing knowledge, even without the tool\\'s help.\\n\\nBut according to the initial instructions, the assistant is supposed to use the provided tools whenever possible. However, in this case, the tool isn\\'t working. The correct approach might be to inform the user that the tool isn\\'t providing results and offer an alternative, but the assistant\\'s role here is to use the tools. Wait, the user\\'s last message was just an empty line after the redirecting responses. The assistant needs to respond to the original query using the tools provided, but since the tool isn\\'t working, maybe the assistant should try rephrasing the query or using another tool. However, the other tool is for Langgraph, which is a different topic.\\n\\nGiven that, perhaps the correct response is to state that the tool isn\\'t providing information and ask the user to rephrase or check the query. But the user might be expecting the assistant to answer the original question using its own knowledge. However, the initial setup says that for questions about LangChain, the assistant must use the tool. Since the tool isn\\'t working, the assistant is stuck. Therefore, the assistant might need to inform the user that the tool is unavailable and cannot retrieve the information, but the assistant still needs to provide an answer based on internal knowledge.\\n\\nWait, but the system\\'s instructions say that the assistant must use the tool for LangChain questions. So if the tool is not working, the assistant can\\'t answer. However, in this case, the user is expecting an answer. The assistant must balance between following the tool usage rules and providing helpful information. Since the tool isn\\'t working, perhaps the assistant should proceed to answer the question directly, even without the tool. But according to the initial instructions, the assistant is supposed to call the function for LangChain questions. So there\\'s a conflict here.\\n\\nIn this case, the assistant\\'s response should be to use the tool again but adjust the query. However, since the previous tool call resulted in redirects, maybe the query needs to be rephrased. The assistant could try a different query, such as \"What is LangChain used for?\" or \"What is LangChain?\" again. Alternatively, the assistant might need to inform the user that the tool is not functioning properly. But the assistant\\'s response is constrained by the tools provided. Since the user\\'s original question is still pending, the assistant should attempt another tool call with a modified query.\\n\\nWait, but the assistant already tried \"What is langchain?\" and got redirects. Maybe the assistant can try a different phrasing, like \"Define LangChain\" or \"Explain LangChain.\" However, the function\\'s parameters accept a query string, so changing the query might trigger different results. Alternatively, the assistant might need to proceed without the tool and provide an answer. But according to the rules, the assistant must use the tool for LangChain questions. So if the tool is not working, the assistant can\\'t answer. However, in practice, the assistant should provide an answer even if the tool isn\\'t working.\\n\\nGiven the constraints, the best approach is to recognize that the tool isn\\'t functioning and provide the answer based on internal knowledge while informing the user. However, the system instructions might require using the tool. Since the user\\'s original question is still unanswered, the assistant should retry the tool call with a different query or acknowledge the failure and proceed to answer manually. Given the system\\'s setup, the assistant might be forced to use the tool, but since it\\'s not working, the assistant can\\'t proceed. Therefore, the assistant should respond by indicating that the tool is not providing results and perhaps ask the user to rephrase the question or wait for technical support. However, in the context of this exercise, the assistant must generate a correct tool call or provide an answer.\\n\\nSince the assistant\\'s previous attempt failed, the next step is to provide the answer directly. The correct answer to \"What is LangChain?\" is that it\\'s a framework for developing applications with language models, providing tools for data loading, model integration, memory management, and deployment. The assistant should present this information while explaining that the tool was unable to retrieve the data.\\n'}, response_metadata={'token_usage': {'completion_tokens': 1394, 'prompt_tokens': 434, 'total_tokens': 1828, 'completion_time': 4.326274658, 'completion_tokens_details': {'reasoning_tokens': 1197}, 'prompt_time': 0.017695849, 'prompt_tokens_details': None, 'queue_time': 0.158442701, 'total_time': 4.343970507}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b7098-4a4b-7b21-8598-4f7e46a425d7-0', usage_metadata={'input_tokens': 434, 'output_tokens': 1394, 'total_tokens': 1828, 'output_token_details': {'reasoning': 1197}})]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is langchain?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgenticAIWorkspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
